
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet/less" type="text/css" href="https://blog.solvik.fr/theme/stylesheet/style.less">
    <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>

  <link rel="stylesheet" type="text/css" href="https://blog.solvik.fr/theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="https://blog.solvik.fr/theme/font-awesome/css/font-awesome.min.css">


    <link href="https://blog.solvik.fr/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom">




    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#3338db">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#3338db">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#3338db">

    <meta name="author" content="Solvik Blum" />
    <meta name="description" content="" />
<meta property="og:site_name" content="A Pelican Blog"/>
<meta property="og:type" content="blog"/>
<meta property="og:title" content="A Pelican Blog"/>
<meta property="og:description" content=""/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://blog.solvik.fr"/>

  <title>A Pelican Blog &ndash;     Tag linux
</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://blog.solvik.fr">
        <img src="https://blog.solvik.fr/theme/img/profile.png" alt="/home/solvik" title="/home/solvik">
      </a>
      <h1><a href="https://blog.solvik.fr">/home/solvik</a></h1>

<p>my place of sharing</p>
      <nav>
        <ul class="list">
          <li><a href="https://blog.solvik.fr/pages/about-me.html#about-me">About @me</a></li>

        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/solvikblum" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/solvik" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-github" href="https://github.com/Solvik" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://blog.solvik.fr">    Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>

      <a href="https://blog.solvik.fr/feeds/all.atom.xml">    Atom
</a>

    </nav>



<article>
  <header>
    <h2><a href="https://blog.solvik.fr/howto-debootstrap.html#howto-debootstrap">How-To Debootstrap</a></h2>
    <p>
          Posted on avril 23, 2014 in <a href="https://blog.solvik.fr/category/system.html">System</a>



    </p>
  </header>
  <div>
      <p>For my infrastructure purposes I often need to install as fast as possible.
Most of my servers comes with 4 disks and one or more RAID card.</p>
<p>I usually don't trust the RAID cards, so I always create a raid0 / disk in order to use every logical volume like it was a real disk.</p>
<p>And I always use the above partition schema</p>
<table>
<thead>
<tr>
<th>mount</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr>
<td>/boot</td>
<td>200M</td>
</tr>
<tr>
<td>/</td>
<td>*</td>
</tr>
</tbody>
</table>
<h2>hpacucli</h2>
<div class="highlight"><pre><span></span><span class="c1"># find your slot</span>
<span class="nv">slot</span><span class="o">=</span><span class="sb">`</span>hpacucli ctrl all show <span class="p">|</span> grep -i slot <span class="p">|</span> awk <span class="s1">&#39;{print $6}&#39;</span>
hpacucli ctrl <span class="nv">slot</span><span class="o">=</span><span class="nv">$slot</span> ld <span class="m">1</span> delete
<span class="c1"># create one raid0 per physical disk</span>
<span class="k">for</span> phys in <span class="sb">`</span>hpacucli ctrl all show config <span class="p">|</span> grep physicaldrive <span class="p">|</span> awk <span class="s1">&#39;{print $2}&#39;</span><span class="sb">`</span><span class="p">;</span>
<span class="k">do</span>
  hpacucli controller <span class="nv">slot</span><span class="o">=</span><span class="nv">$slot</span> create <span class="nv">type</span><span class="o">=</span>ld <span class="nv">drives</span><span class="o">=</span><span class="nv">$phys</span> <span class="nv">raid</span><span class="o">=</span><span class="m">0</span>
<span class="k">done</span><span class="p">;</span>
</pre></div>


<h2>Cleaning</h2>
<p>If you use an old server, you must do some cleaning</p>
<p>Let's start by zeroing the first 100MB of the partition in order to be sure to erase the partition table, the MBR</p>
<div class="highlight"><pre><span></span><span class="k">for</span> i in <span class="o">{</span>a..d<span class="o">}</span> <span class="p">;</span>
<span class="k">do</span>
  dd <span class="k">if</span><span class="o">=</span>/dev/sda <span class="nv">of</span><span class="o">=</span>/dev/zero <span class="nv">count</span><span class="o">=</span><span class="m">100</span> <span class="nv">bs</span><span class="o">=</span>1M
<span class="k">done</span>
</pre></div>


<p>Afterwars, let's notify the kernel about devices changes</p>
<div class="highlight"><pre><span></span>partprobe
</pre></div>


<h2>MSDOS partitions</h2>
<div class="highlight"><pre><span></span><span class="k">for</span> i in <span class="o">{</span>a..d<span class="o">}</span> <span class="p">;</span>
<span class="k">do</span>
  parted /dev/sd<span class="nv">$i</span> --script -- mklabel msdos
  parted /dev/sd<span class="nv">$i</span> -a optimal --script -- unit MB mkpart primary <span class="m">1</span> <span class="m">200</span>
  parted /dev/sd<span class="nv">$i</span> -a optimal --script -- unit MB mkpart primary <span class="m">200</span> -1
<span class="k">done</span><span class="p">;</span>
</pre></div>


<h2>GPT partitions</h2>
<p>For GPT partitions you need to create <a href="http://en.wikipedia.org/wiki/BIOS_Boot_partition">BIOS Boot partition</a> a small partition, at least 1mb.</p>
<p>&nbsp;</p>
<div class="highlight"><pre><span></span><span class="k">for</span> i in <span class="o">{</span>a..d<span class="o">}</span> <span class="p">;</span>
<span class="k">do</span>
    parted /dev/sd<span class="nv">$i</span> --script -- mklabel gpt
    parted /dev/sd<span class="nv">$i</span> -a optimal --script -- unit MB mkpart grub fat32 1mb 2mb
    parted /dev/sd<span class="nv">$i</span> -a optimal --script -- unit MB <span class="nb">set</span> <span class="m">1</span> bios_grub on
    parted /dev/sd<span class="nv">$i</span> -a optimal --script -- unit MB mkpart primary 2mb <span class="m">200</span>
    parted /dev/sd<span class="nv">$i</span> -a optimal --script -- unit MB mkpart primary <span class="m">200</span> -1
<span class="k">done</span><span class="p">;</span>
</pre></div>


<h2>Installation</h2>
<p>I prefer to use software raid with mdadm.
If you want to boot on a mdadm's volume you need it to use the <a href="https://raid.wiki.kernel.org/index.php/RAID_superblock_formats">0.90 metadatas</a>
For you <strong>/</strong>, use the raid-level you want and don't give any metadata paramaters so it can takes the <strong>1.2</strong> one.</p>
<p><strong>/!\</strong> If you use GPT partitions, be aware that <strong>/dev/sdx1</strong> is the BIOS partition, not your future /boot, start at <strong>/dev/sdx2</strong></p>
<div class="highlight"><pre><span></span><span class="c1"># for msdos partitions</span>
mdadm --create /dev/md0 --metadata<span class="o">=</span><span class="m">0</span>.90 --assume-clean --raid-devices<span class="o">=</span><span class="m">4</span> --level<span class="o">=</span><span class="m">1</span> /dev/sda1 /dev/sdb1 /dev/sdc1 /dev/sdd1
mdadm --create /dev/md1 --assume-clean --raid-devices<span class="o">=</span><span class="m">4</span> --level<span class="o">=</span><span class="m">6</span> /dev/sda2 /dev/sdb2 /dev/sdc2 /dev/sdd2

<span class="c1"># for gpt partitions</span>
mdadm --create /dev/md0 --metadata<span class="o">=</span><span class="m">0</span>.90 --assume-clean --raid-devices<span class="o">=</span><span class="m">4</span> --level<span class="o">=</span><span class="m">1</span> /dev/sda2 /dev/sdb2 /dev/sdc2 /dev/sdd2
mdadm --create /dev/md1 --assume-clean --raid-devices<span class="o">=</span><span class="m">4</span> --level<span class="o">=</span><span class="m">6</span> /dev/sda3 /dev/sdb3 /dev/sdc3 /dev/sdd3
</pre></div>


<p>Let's format the RAID volumes</p>
<div class="highlight"><pre><span></span>mkfs.ext4 /dev/md0
mkfs.ext4 /dev/md1
</pre></div>


<p>Let's start the debootstrap session. I use a basic /etc/apt/sources.list using this <a href="http://repogen.simplylinux.ch/">convenient sources.list generator</a></p>
<div class="highlight"><pre><span></span>mkdir /mnt/root
mount /dev/md1 /mnt/root
apt-get update<span class="p">;</span> apt-get install -y debootstrap
debootstrap trusty /mnt/root
mount -o <span class="nb">bind</span> /dev /mnt/root/dev
mount -o <span class="nb">bind</span> /proc /mnt/root/proc
mount -o <span class="nb">bind</span> /sys /mnt/root/sys

<span class="c1"># basic fstab</span>
<span class="nb">echo</span> <span class="s2">&quot;proc            /proc   proc    defaults                0       0</span>
<span class="s2">/dev/md1 /       ext4    errors=remount-ro       0       1</span>
<span class="s2">/dev/md0        /boot   ext4    defaults                0       2</span>
<span class="s2">&quot;</span> <span class="p">&amp;</span>gt<span class="p">;</span> /mnt/root/etc/fstab

<span class="nb">echo</span> <span class="s2">&quot;#############################################################</span>
<span class="s2">################### OFFICIAL UBUNTU REPOS ###################</span>
<span class="s2">#############################################################</span>

<span class="s2">###### Ubuntu Main Repos</span>
<span class="s2">deb http://fr.archive.ubuntu.com/ubuntu/ trusty main restricted universe multiverse</span>
<span class="s2">deb-src http://fr.archive.ubuntu.com/ubuntu/ trusty main restricted universe multiverse</span>

<span class="s2">###### Ubuntu Update Repos</span>
<span class="s2">deb http://fr.archive.ubuntu.com/ubuntu/ trusty-security main restricted universe multiverse</span>
<span class="s2">deb http://fr.archive.ubuntu.com/ubuntu/ trusty-updates main restricted universe multiverse</span>
<span class="s2">deb-src http://fr.archive.ubuntu.com/ubuntu/ trusty-security main restricted universe multiverse</span>
<span class="s2">deb-src http://fr.archive.ubuntu.com/ubuntu/ trusty-updates main restricted universe multiverse</span>
<span class="s2">&quot;</span> <span class="p">&amp;</span>gt<span class="p">;</span> /mnt/root/etc/apt/sources.list
</pre></div>


<p>Now we can go the installed volume and prepare the OS</p>
<div class="highlight"><pre><span></span><span class="nb">cd</span> /mnt/root
chroot .
<span class="c1"># mount /boot for the future kernel installation</span>
mount /boot
<span class="c1"># generate a few locales</span>
locale-gen fr_FR.UTF-8
locale-gen fr_FR
locale-gen en_US.UTF-8
locale-gen en_US
update-locale

apt-get update
<span class="c1"># don&#39;t forget to install mdadm on the system so it can boots correctly</span>
apt-get install -y mdadm lvm2
<span class="c1"># install the required kernel</span>
apt-get install -y linux-image-generic
<span class="c1"># install an openssh-server so you can remotely have access to the system</span>
apt-get install -y openssh-server
<span class="c1"># change your root password!!</span>
<span class="nb">echo</span> <span class="s2">&quot;root:changeme&quot;</span><span class="p">|</span>chpasswd
</pre></div>


<p>Stop the few services</p>
<div class="highlight"><pre><span></span>/etc/init.d/ssh stop
</pre></div>


<p>Umount everything, sync for the last i/o and reboot</p>
<div class="highlight"><pre><span></span>umount /boot
<span class="nb">exit</span>
umount /mnt/root/dev
umount /mnt/root/proc
umount /mnt/root/sys
sync
reboot
</pre></div>


<h2>LVM</h2>
<p>Work in progress</p>
<h2>Rescue</h2>
<p>Without LVM</p>
<h4></h4>
<p>If you happen to boot on a rescue live-cd on one of this configuration, it will detect a RAID system but without the correct device names</p>
<div class="highlight"><pre><span></span>mdadm -S /dev/md126
mdadm -S /dev/md127
mdadm --examine --scan /dev/sda<span class="o">{</span><span class="m">1</span>..4<span class="o">}</span> <span class="p">&amp;</span>gt<span class="p">;&amp;</span>gt<span class="p">;</span> /etc/mdadm/mdadm.conf
mdadm --assemble --scan
</pre></div>


<p>Your <strong>/dev/md0</strong> and <strong>/dev/md1</strong> should come online</p>
<div class="highlight"><pre><span></span>mkdir -p /mnt/root
mount /dev/md1 /mnt/root
mount -o <span class="nb">bind</span> /dev /mnt/root/dev
mount -o <span class="nb">bind</span> /proc /mnt/root/proc
mount -o <span class="nb">bind</span> /sys /mnt/root/sys
chroot /mnt/root
</pre></div>


<p>Here you go!</p>
<h2>Credits</h2>
<p>Thanks to my friends <a href="http://www.si7v.fr">Pierre Tourbeaux</a> and <a href="http://www.ipsolution.fr">Michael Kozma</a> for all the advices and debugging over the year :)</p>
      <br>
      <a class="btn" href="https://blog.solvik.fr/howto-debootstrap.html#howto-debootstrap">    Continue reading
</a>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="https://blog.solvik.fr/zfs-on-linux.html#zfs-on-linux">ZFSonLinux</a></h2>
    <p>
          Posted on avril 18, 2014 in <a href="https://blog.solvik.fr/category/system.html">System</a>



    </p>
  </header>
  <div>
      <p>At Online, we've been trying <a href="http://zfsonlinux.org/">ZFS On Linux</a> on a few services.</p>
<p>Here's a small how-to (and also a reminder) on how to install it and manage it:</p>
<h2>Install</h2>
<div class="highlight"><pre><span></span>    $ apt-add-repository --yes ppa:zfs-native/stable
    $ apt-get update &amp;amp;&amp;amp; apt-get install ubuntu-zfs
</pre></div>


<p>ZFS comes with a RAID soft like system</p>
<table>
<thead>
<tr>
<th>RAID type</th>
<th>RAID-z type</th>
<th align="center">Loosable disks</th>
<th>Min disks</th>
</tr>
</thead>
<tbody>
<tr>
<td>RAID5</td>
<td>raidz</td>
<td align="center">1 disks</td>
<td>3 disks</td>
</tr>
<tr>
<td>RAID6</td>
<td>raidz-2</td>
<td align="center">2 disks</td>
<td>4 disks</td>
</tr>
<tr>
<td>RAID7</td>
<td>raidz-3</td>
<td align="center">3 disks</td>
<td>5 disks</td>
</tr>
</tbody>
</table>
<p>Now we're going to create a zpool called <strong>storage</strong></p>
<div class="highlight"><pre><span></span>    $ zpool create -f storage raidz2 c2d{1..5}
</pre></div>


<p>If we wan't to add MOAR disks</p>
<div class="highlight"><pre><span></span>    $ zpool add -f storage raidz2 c2d{6..10}
</pre></div>


<p>Here are a few problems I've experienced:</p>
<h2>ZFS Resilvering (replace a drive)</h2>
<p>If you've got some spare disks, you should add them your spare pool</p>
<div class="highlight"><pre><span></span>    $ zpool add storage spare c2d11 c2d12
</pre></div>


<p>By doing so, if a disk fails, ZFS will replace it automatically with the failed one.
Personnaly, I prefer to do it manualy.
Assuming c2d4 failed, to replace it by c2d11, let's do this:</p>
<div class="highlight"><pre><span></span>    $ zpool replace c2d4 c2d11
</pre></div>


<p>You will now have c2d11 resilvering your entire zpool. Once the resilver ends, the failed disk is ejected from the zpool.</p>
<h2>ZFS Scrubbing</h2>
<p>ZFS has a scrub feature to detect and correct silently errors. You could assimilate this to ECC RAM (RAM with error recovery).
ZFS scrub feature check every block of your pool against a SHA-256 checksum.</p>
<p>You can invoke a scrub or be forced to live the scrub when a disk fails and you have to replace it.</p>
<p>Recently, on a 200T system, I replaced a failed disk by a spare one. It scrubbed the 200T.
The <strong>zpool status</strong> was mentionning a duration of about 500 hours of scrubbing. Time to hang yourself.</p>
<p>Fortunately, there is some tunnable settings in <strong>/sys/module/zfs/parameters</strong></p>
<div class="highlight"><pre><span></span>    # Prioritize resilvering by setting the delay to zero
    $ echo 0 &amp;gt; zfs_resilver_delay

    # Prioritize scrubs by setting the delay to zero
    $ echo 0 &amp;gt; zfs_scrub_delay
</pre></div>


<p>These changes takes effect immediatly and I haven't experienced any problems afterwards. Everything synced in 60 hours.</p>
<p>Attached a few other features to tune your scrub:</p>
<table>
<thead>
<tr>
<th>feature</th>
<th>default value</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>zfs_top_maxinflight</td>
<td>32</td>
<td>maximum I/Os per top-level</td>
</tr>
<tr>
<td>zfs_resilver_delay</td>
<td>2</td>
<td>number of ticks to delay resilver</td>
</tr>
<tr>
<td>zfs_scrub_delay</td>
<td>4</td>
<td>number of ticks to delay scrub</td>
</tr>
<tr>
<td>zfs_scan_idle</td>
<td>50</td>
<td>idle window in clock ticks</td>
</tr>
<tr>
<td>zfs_scan_min_time_ms</td>
<td>1000</td>
<td>min millisecs to scrub per txg</td>
</tr>
<tr>
<td>zfs_free_min_time_ms</td>
<td>1000</td>
<td>min millisecs to free per txg</td>
</tr>
<tr>
<td>zfs_resilver_min_time_ms</td>
<td>3000</td>
<td>min millisecs to resilver per txg</td>
</tr>
<tr>
<td>zfs_no_scrub_io</td>
<td>0 (bool)</td>
<td>set to disable scrub i/o</td>
</tr>
<tr>
<td>zfs_no_scrub_prefetch</td>
<td>0 (bool)</td>
<td>set to disable srub prefetching</td>
</tr>
</tbody>
</table>
<h2>Links</h2>
<ul>
<li><a href="http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers/">https://pthree.org/2012/12/11/zfs-administration-part-vi-scrub-and-resilver/</a></li>
<li><a href="http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers/">http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers/</a></li>
</ul>
      <br>
      <a class="btn" href="https://blog.solvik.fr/zfs-on-linux.html#zfs-on-linux">    Continue reading
</a>
  </div>
</article>

  <div class="pagination">
  </div>




    <footer>
<p>
  &copy;  2018 - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " A Pelican Blog ",
  "url" : "https://blog.solvik.fr",
  "image": "",
  "description": ""
}
</script>

</body>
</html>